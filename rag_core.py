# rag_core.py (TF-IDF è½»é‡ç‰ˆï¼Œæ— éœ€HuggingFaceå’Œtorch)
#
# æ€è·¯ï¼š
# 1. æŠŠä¸Šä¼ çš„æ–‡æ¡£åˆ‡æˆchunk
# 2. ç”¨ TfidfVectorizer æŠŠæ‰€æœ‰chunkå‘é‡åŒ–
# 3. ç”¨æˆ·æé—® -> ä¹Ÿå‘é‡åŒ– -> è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
# 4. å–æœ€ç›¸ä¼¼çš„ç‰‡æ®µï¼Œç»„æˆå›ç­”
#
# è¿™æ ·ä¾ç„¶æ˜¯â€œåŸºäºæˆ‘ä¸Šä¼ çš„æ–‡æ¡£å›ç­”â€ï¼Œç¬¦åˆRAGé€»è¾‘ï¼Œ
# è€Œä¸”ä¸ç”¨huggingfaceæ¨¡å‹ï¼Œæ‰€ä»¥Streamlit Cloudä¸ä¼šæŠ¥ImportErrorã€‚

from typing import List, Tuple
import io
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from langchain_text_splitters import CharacterTextSplitter


def load_file_to_text(file_bytes: bytes, filename: str) -> str:
    """
    ç®€å•è¯»å–æ–‡æœ¬å‹æ–‡ä»¶ã€‚å¯¹PDFç­‰å¤æ‚æ ¼å¼æš‚æ—¶ä¸åšOCRï¼Œåªå°è¯•ç›´æ¥decodeã€‚
    """
    lower_name = filename.lower()

    # ç›´æ¥æŒ‰utf-8è¯»
    try:
        text = file_bytes.decode("utf-8", errors="ignore")
        return text
    except Exception:
        pass

    # å¤‡ç”¨ç¼–ç 
    try:
        text = file_bytes.decode("cp949", errors="ignore")
        return text
    except Exception:
        pass

    return ""


def split_text_to_chunks(text: str,
                         chunk_size: int = 500,
                         chunk_overlap: int = 100) -> List[str]:
    """
    æŠŠé•¿æ–‡æœ¬åˆ‡æˆå°æ®µï¼Œä¿ç•™ä¸€å®šé‡å ï¼Œæ–¹ä¾¿æ£€ç´¢ã€‚
    """
    splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len
    )
    chunks = splitter.split_text(text)
    return chunks


class SimpleVectorStore:
    """
    ä¸€ä¸ªå¾ˆè½»é‡çš„å‘é‡åº“ï¼š
    - ç”¨ TF-IDF æŠŠæ‰€æœ‰chunkç¼–ç æˆå‘é‡çŸ©é˜µ
    - åšä½™å¼¦ç›¸ä¼¼åº¦æ£€ç´¢
    """

    def __init__(self, chunks: List[str]):
        self.chunks = chunks  # æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
        self.vectorizer = TfidfVectorizer()
        if chunks:
            self.matrix = self.vectorizer.fit_transform(chunks)  # shape: (num_chunks, vocab_dim)
        else:
            self.matrix = None

    def similarity_search(self, query: str, k: int = 3) -> List[Tuple[str, float]]:
        """
        è¿”å›ä¸queryæœ€ç›¸ä¼¼çš„kä¸ªchunkï¼Œé™„å¸¦ç›¸ä¼¼åº¦åˆ†æ•°ã€‚
        """
        if (self.matrix is None) or (not query.strip()):
            return []

        q_vec = self.vectorizer.transform([query])  # shape: (1, vocab_dim)

        # ä½™å¼¦ç›¸ä¼¼åº¦ = (A Â· B) / (||A||*||B||)
        # è¿™é‡Œä½¿ç”¨ç¨€ç–çŸ©é˜µä¹˜æ³•å¾—åˆ°ç‚¹ç§¯ï¼Œå†é™¤ä»¥èŒƒæ•°
        dot_scores = (self.matrix @ q_vec.T).toarray().ravel()  # (num_chunks,)
        doc_norms = np.linalg.norm(self.matrix.toarray(), axis=1) + 1e-10
        q_norm = np.linalg.norm(q_vec.toarray()) + 1e-10
        cosine_scores = dot_scores / (doc_norms * q_norm)

        # æ’åºï¼Œå–top k
        idx_sorted = np.argsort(cosine_scores)[::-1]  # ä»å¤§åˆ°å°
        top_idx = idx_sorted[:k]

        results = []
        for i in top_idx:
            results.append((self.chunks[i], float(cosine_scores[i])))

        return results


def build_vectorstore_from_chunks(chunks: List[str]):
    """
    ä½¿ç”¨SimpleVectorStoreï¼Œè€Œä¸æ˜¯HuggingFaceEmbeddings+Chromaã€‚
    """
    if not chunks:
        return None
    return SimpleVectorStore(chunks)


def build_answer_from_passages(query: str,
                               passages: List[Tuple[str, float]]) -> str:
    """
    æ ¹æ®æ£€ç´¢åˆ°çš„ç‰‡æ®µï¼Œç»„åˆä¸€ä¸ª"åŸºäºä½ çš„æ–‡æ¡£"çš„å›ç­”ã€‚
    ä¸è°ƒç”¨OpenAIï¼Œå®Œå…¨å…è´¹ã€‚
    """
    if not passages:
        return (
            "ğŸ“˜ æ²¡æ‰¾åˆ°å’Œé—®é¢˜å¼ºç›¸å…³çš„å†…å®¹ã€‚\n"
            "å¯èƒ½è¿˜æ²¡æœ‰æˆåŠŸè§£æè¿™ä¸ªæ–‡ä»¶ï¼Œæˆ–è€…æ–‡æ¡£å†…å®¹å’Œæé—®å·®è·å¤ªå¤§ã€‚ğŸ"
        )

    answer_lines = []
    answer_lines.append("ğŸ ä½ çš„é—®é¢˜ï¼š " + query.strip())
    answer_lines.append("")
    answer_lines.append("ğŸ“š æ ¹æ®ä½ ä¸Šä¼ çš„æ–‡æ¡£ï¼Œæœ€ç›¸å…³çš„å†…å®¹æ˜¯ï¼š")

    for idx, (text_block, score) in enumerate(passages, start=1):
        preview = text_block.strip()
        if len(preview) > 400:
            preview = preview[:400] + " ..."
        answer_lines.append(f"\n[{idx}] ç›¸ä¼¼åº¦ {score:.3f}\n{preview}")

    answer_lines.append(
        "\nâœ¿ æç¤ºï¼šä»¥ä¸Šå›ç­”åªæ¥è‡ªä½ ä¸Šä¼ çš„èµ„æ–™ï¼ˆæœ¬åœ°æ£€ç´¢ï¼‰ï¼Œ"
        "å¹¶ä¸æ˜¯äº’è”ç½‘é€šç”¨çŸ¥è¯†ã€‚\n"
    )

    return "\n".join(answer_lines)


class RAGSessionState:
    """
    ä¿å­˜ä¼šè¯çŠ¶æ€ï¼š
    - æ‰€æœ‰åŸå§‹æ–‡æœ¬
    - åˆ‡åˆ†åçš„chunks
    - ä¸€ä¸ªTF-IDFå‘é‡åº“
    """
    def __init__(self):
        self.raw_texts = []
        self.all_chunks = []
        self.vectorstore = None

    def add_document(self, file_bytes: bytes, filename: str):
        """
        æ·»åŠ æ–°æ–‡ä»¶åï¼Œé‡æ–°æ„å»ºchunkså’Œå‘é‡åº“ï¼ˆç®€å•ç²—æš´ç‰ˆï¼Œè¶³å¤Ÿå­¦ç”Ÿä½œä¸šï¼‰ã€‚
        """
        text = load_file_to_text(file_bytes, filename)
        if text.strip():
            self.raw_texts.append(text)

        merged = "\n\n".join(self.raw_texts)
        chunks = split_text_to_chunks(merged)
        self.all_chunks = chunks
        self.vectorstore = build_vectorstore_from_chunks(chunks)

    def ask(self, query: str) -> str:
        if self.vectorstore is None:
            return "è¿˜æ²¡æœ‰å¯æ£€ç´¢çš„å†…å®¹ï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£ ğŸ"
        passages = self.vectorstore.similarity_search(query, k=3)
        answer = build_answer_from_passages(query, passages)
        return answer
