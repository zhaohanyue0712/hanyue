# rag_core.py
#
# RAG í•µì‹¬ ë¡œì§:
# - ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ í…ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ/ë¶„í• 
# - ì„ë² ë”©ìœ¼ë¡œ ë²¡í„°DB ìƒì„± (Chroma)
# - ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê°€ì¥ ë¹„ìŠ·í•œ ë¬¸ì„œì¡°ê°ì„ ì°¾ì•„ì„œ ë‹µë³€ í›„ë³´ ìƒì„±
#
# OpenAI ê²°ì œ ì—†ì´ ë™ì‘í•˜ë„ë¡ sentence-transformers ê¸°ë°˜ ì„ë² ë”© ì‚¬ìš©.

import io
from typing import List, Tuple
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings


def load_file_to_text(file_bytes: bytes, filename: str) -> str:
    """
    ì—…ë¡œë“œëœ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜.
    í˜„ì¬ëŠ” .txt / .md / .csv / .py ë“± 'í…ìŠ¤íŠ¸ ê¸°ë°˜' íŒŒì¼ ìœ„ì£¼ë¡œ ì²˜ë¦¬.
    PDF ë“± ë³µì¡í•œ í¬ë§·ì€ ì—¬ê¸°ì„œ í™•ì¥ ê°€ëŠ¥.
    """
    lower_name = filename.lower()
    # ê°€ì¥ ë‹¨ìˆœí•œ ë°©ì‹: ê·¸ëƒ¥ utf-8 ë””ì½”ë”© ì‹œë„
    try:
        text = file_bytes.decode("utf-8", errors="ignore")
        return text
    except Exception:
        pass

    # í˜¹ì‹œ ëª¨ë¥¼ ë‹¤ë¥¸ ì¸ì½”ë”©
    try:
        text = file_bytes.decode("cp949", errors="ignore")
        return text
    except Exception:
        pass

    # ë§ˆì§€ë§‰ fallback: ë°”ì´ë„ˆë¦¬ -> ë¹ˆ ë¬¸ìì—´
    return ""


def split_text_to_chunks(text: str,
                         chunk_size: int = 500,
                         chunk_overlap: int = 100) -> List[str]:
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°.
    chunk_size / chunk_overlap ê°’ì€ ë„ˆë¬´ ê³µê²©ì ìœ¼ë¡œ í‚¤ìš°ì§€ ì•ŠìŒ (í•™ìƒ ë‚œì´ë„).
    """
    splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len
    )
    chunks = splitter.split_text(text)
    return chunks


def build_vectorstore_from_chunks(chunks: List[str]):
    """
    ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì„œ Chroma ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë©”ëª¨ë¦¬ ìƒì— ìƒì„±.
    HuggingFace ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© â†’ ë¬´ë£Œ.
    ëª¨ë¸ì€ ì†Œí˜• SBERT ê³„ì—´ì„ ì‚¬ìš©í•´ í•™ìƒ í™˜ê²½ì—ì„œë„ ë¹„êµì  ê°€ë³ê²Œ ë™ì‘.
    """
    if not chunks:
        return None

    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    vectorstore = Chroma.from_texts(
        texts=chunks,
        embedding=embedding_model
    )
    return vectorstore


def retrieve_similar_passages(
    vectorstore,
    query: str,
    k: int = 3
) -> List[Tuple[str, float]]:
    """
    ì‚¬ìš©ìì˜ ì§ˆë¬¸(query)ê³¼ ìœ ì‚¬í•œ ìƒìœ„ kê°œ ë¬¸ë‹¨ì„ ê²€ìƒ‰.
    ë°˜í™˜: [(ë¬¸ë‹¨ë‚´ìš©, ìœ ì‚¬ë„ ì ìˆ˜), ...]
    """
    if vectorstore is None:
        return []

    docs_and_scores = vectorstore.similarity_search_with_score(query, k=k)

    results = []
    for doc, score in docs_and_scores:
        results.append((doc.page_content, score))
    return results


def build_answer_from_passages(query: str,
                               passages: List[Tuple[str, float]]) -> str:
    """
    OpenAI ìœ ë£Œ API ì—†ì´ ë‹µë³€ì„ "ìƒì„±"í•˜ëŠ” ë°©ì‹.
    - ìƒìœ„ ê´€ë ¨ ë¬¸ë‹¨ë“¤ì„ ë½‘ì•„ì„œ ìš”ì•½ í˜•íƒœë¡œ ë³´ì—¬ì¤€ë‹¤.
    - 'ê·¼ê±° ê¸°ë°˜ ë‹µë³€'ì²˜ëŸ¼ ë³´ì´ë„ë¡ êµ¬ì„±.
    """
    if not passages:
        return (
            "ğŸ“˜ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"
            "ì—…ë¡œë“œí•œ ë¬¸ì„œì— í•´ë‹¹ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë‚´ìš©ì´ ê±°ì˜ ì—†ê±°ë‚˜\n"
            "ì•„ì§ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆì–´ìš”. ğŸ"
        )

    answer_lines = []
    answer_lines.append("ğŸ ì§ˆë¬¸: " + query.strip())
    answer_lines.append("")
    answer_lines.append("ğŸ“š ë¬¸ì„œì—ì„œ ì°¾ì€ ê´€ë ¨ ë‚´ìš© ìš”ì•½:")

    for idx, (text_block, score) in enumerate(passages, start=1):
        # ë„ˆë¬´ ê¸´ ë¸”ë¡ì„ í•œ ë²ˆ ë” ì˜ë¼ì„œ ê¹”ë”í•˜ê²Œ
        short_preview = text_block.strip()
        if len(short_preview) > 400:
            short_preview = short_preview[:400] + " ..."

        answer_lines.append(f"\n[{idx}] {short_preview}")

    answer_lines.append(
        "\nâœ¿ ìœ„ ë‚´ìš©ì€ ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ì§ì ‘ ê²€ìƒ‰ëœ ê·¼ê±°ì…ë‹ˆë‹¤.\n"
        "âœ¿ ì¦‰, ì´ ì±—ë´‡ì€ ì¼ë°˜ì ì¸ ì§€ì‹ì´ ì•„ë‹ˆë¼ 'ë‚´ ìë£Œ'ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µí•´ìš”.\n"
    )

    return "\n".join(answer_lines)


class RAGSessionState:
    """
    Streamlit ì„¸ì…˜ê³¼ ì—°ê²°í•´ì„œ ì“¸ ì‘ì€ ìƒíƒœ ê´€ë¦¬ìš© í´ë˜ìŠ¤.
    - ì—…ë¡œë“œ ë¬¸ì„œ ì „ì²´ í…ìŠ¤íŠ¸
    - ì˜ë¼ë‚¸ ì²­í¬
    - ë²¡í„°ìŠ¤í† ì–´
    """
    def __init__(self):
        self.raw_texts = []        # ì›ë³¸ í…ìŠ¤íŠ¸ë“¤ (íŒŒì¼ë³„)
        self.all_chunks = []       # ì˜ë¦° ì²­í¬ ì „ì²´
        self.vectorstore = None    # Chroma ë²¡í„°ìŠ¤í† ì–´

    def add_document(self, file_bytes: bytes, filename: str):
        """
        ìƒˆ íŒŒì¼ì„ ì„¸ì…˜ì— ì¶”ê°€í•˜ê³ , ì „ì²´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë‹¤ì‹œ ë¹Œë“œí•œë‹¤.
        (ê°„ë‹¨í•˜ê²Œ 'ë®ì–´ì“°ê¸°' ì‹ìœ¼ë¡œ ì¬êµ¬ì„±)
        """
        text = load_file_to_text(file_bytes, filename)
        if text.strip():
            self.raw_texts.append(text)

        # ëª¨ë“  ë¬¸ì„œë¥¼ í•©ì³ì„œ ë‹¤ì‹œ ì²­í¬í™”
        merged = "\n\n".join(self.raw_texts)
        chunks = split_text_to_chunks(merged)
        self.all_chunks = chunks
        self.vectorstore = build_vectorstore_from_chunks(chunks)

    def ask(self, query: str) -> str:
        """
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ RAG ê²€ìƒ‰ í›„ ìš”ì•½í˜• ë‹µë³€ ìƒì„±.
        """
        passages = retrieve_similar_passages(self.vectorstore, query)
        answer = build_answer_from_passages(query, passages)
        return answer
